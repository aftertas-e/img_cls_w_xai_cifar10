# 프로젝트의 목적
- 딥러닝(DL)은 인공신경망(ANN)을 깊게 쌓아 모델을 제작, 학습, 추론하는 메커니즘으로, 기존의 다른 알고리즘과 비교했을 때 우수한 성능을 지니고 있다.
-   그러나 딥러닝은 작동 방식이 분명하게 증명되지 않은 블랙박스의 형태다.
- 따라서 타인을 납득시킬 올바른 모델을 설계하려면, 단순히 모델로 추론 결과를 내는 것보다 **왜** 그런 결과가 나왔는지 설명할 수 있어야 한다.
-   이를 설명 가능한 인공지능, XAI(eXplainable AI)로 부르며, 이를 위한 연구는 모델의 복잡도(Complexity), 설명 범주(Scope), 모델 특화 정도(Dependency)의 기준 등으로 분류되어 진행되고 있다. 
- 본 과제에서는 대표적인 데이터 세트인 CIFAR-10으로 여러 모델을 학습하고, 해당 모델을 모델 특화 정도(Dependency) 측면에서 설명한다.
-   여기서 모델 특화 정도란, 모델의 설명 방법이 특정 종류의 모델만 적용할 수 있는지, 범용적으로 설명 가능한지에 따라 각각 Model-specific과 Model-agnostic으로 나눌 수 있다.
-   본 보고에서는 구체적으로 Model-specific에 초점을 두며, **CNN에서 사용할 수 있는 시각화 기법(Intermediate Feature Space 및 CAM)을 활용**하여 모델의 추론 과정을 설명한다. 
